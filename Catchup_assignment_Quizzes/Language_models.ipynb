{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOONEH/IVeepMLTCVASggwI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukkatharun/advance-deep-learning-assignments/blob/main/Catchup_assignment_Quizzes/Language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv-jAG9K4gRG",
        "outputId": "d97d6e66-c59a-4e60-9b1b-704868f2ea9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.25.0.tar.gz (44 kB)\n",
            "\u001b[K     |████████████████████████████████| 44 kB 2.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.8/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from openai) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai) (1.21.6)\n",
            "Collecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.5.2.221124-py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Collecting types-pytz>=2022.1.1\n",
            "  Downloading types_pytz-2022.6.0.1-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.9.24)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.25.0-py3-none-any.whl size=55880 sha256=56a716d7bb0d1dd98af29a542871445f582554f7442cf6bc2874b5a00731897c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/92/33/6f57c7aae0b16875267999a50570e81f15eecec577ebe05a2e\n",
            "Successfully built openai\n",
            "Installing collected packages: types-pytz, pandas-stubs, openai\n",
            "Successfully installed openai-0.25.0 pandas-stubs-1.5.2.221124 types-pytz-2022.6.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai"
      ],
      "metadata": {
        "id": "WkGlSS3j4qbF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using the GPT 3 Model which is trained on 175 BN ML parameters."
      ],
      "metadata": {
        "id": "Dd_ksi44LpPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Connecting with Open AI's GPT3\n",
        "\n",
        "In this step we will connect with GPT 3 models and pass some of them parameters it requires.\n",
        "\n",
        "let's understand what each of these parameters does:\n",
        "\n",
        "**max_token**: this is the word limit provided as output\n",
        "\n",
        "**temparature**: value shoulde be between 0 & 1. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.\n",
        "\n",
        "**engine**: this is set to the “text-davinci-002”, which is the “most capable” GPT-3 model based on OpenAI’s documentation.\n",
        "\n",
        "**top_p** sets out the distribution to select the outputs from. Using the same example above, a top_p of 0.75 tells the model to only select word A as it’s the only word with a probability exceeding 0.75.\n",
        "\n",
        "**frequency_penalty** and **presence_penalty** both are parameters which penalise the model for returning outputs which appear often.\n",
        "\n",
        "please create an account with Open AI to access the GPT 3 model [link](https://openai.com/api/)"
      ],
      "metadata": {
        "id": "YHyWWpRp9TH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def promptGeneration(texts):\n",
        "## Call the API key under your account (in a secure way)\n",
        "  openai.api_key = \"personal token\" #please create the api key in the Open AI's platform and use it here\n",
        "  response = openai.Completion.create(\n",
        "  engine=\"text-davinci-002\",\n",
        "  prompt =  texts,\n",
        "  temperature = 0.6,\n",
        "  top_p = 1,\n",
        "  max_tokens = 128,\n",
        "  frequency_penalty = 0,\n",
        "  presence_penalty = 0\n",
        "  )\n",
        "  return print(response.choices[0].text)"
      ],
      "metadata": {
        "id": "XdTeiKdl4uis"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarization Usecase\n",
        "\n",
        "Asking the Language model to summarize the paragraph given"
      ],
      "metadata": {
        "id": "IxmtJrFO9rZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarization = \"\"\"Summarize this for a second-grade student: Jupiter is the fifth planet from the Sun and the largest in the Solar System. \n",
        "It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. \n",
        "Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. \n",
        "It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows, \n",
        "[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.\"\"\"\n",
        "\n",
        "promptGeneration(summarization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiRd3hNk4_PK",
        "outputId": "875d10a4-2fc2-4d11-d3bd-0584fd980906"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Jupiter is the fifth planet from the Sun, and it is the largest planet in the Solar System. It is huge! Jupiter is two-and-a-half times the size of all the other planets in the Solar System combined. It is so bright that it can cast shadows. Jupiter is named after\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarization = \"\"\"As data science and machine learning adoption has grown over the last few years, \n",
        "Python is catching up to SQL in popularity within the world of data processing. SQL and Python are both powerful on their own, \n",
        "but their value in modern analytics is highest when they work together. This was a key motivator for us at Snowflake to build Snowpark \n",
        "for Python to help modern analytics, data engineering, data developers, and data science teams generate insights without complex infrastructure management \n",
        "for separate languages\"\"\"\n",
        "\n",
        "promptGeneration(summarization)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b41AYoDrEF3o",
        "outputId": "13c2de30-3451-49cc-d225-e801a0f7438f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", platforms, and tools. \n",
            "\n",
            "Snowpark is a managed Python runtime that works with your existing Snowflake account and data. You can use it to develop, test, \n",
            "and deploy Python-based data applications directly in Snowflake. And because Snowpark is a managed runtime, you don’t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Translation Use case\n",
        "\n",
        "Using the Language Model to translate from english to different languages"
      ],
      "metadata": {
        "id": "tY9fyGtZ9uaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"English: 'Hello!', Telugu: \"\n",
        "promptGeneration(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_keE6UZCHiRj",
        "outputId": "f96107e6-1f0b-4c1d-9e77-e019262fb45e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "హలో!},\n",
            "  {English: 'How are you?', Telugu: మీరు ఎలా ఉన్నారో?},\n",
            "  {English: 'Fine, thank you.', Telugu: సరే, ధన్యవాదాల\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"English: 'Hello!', Chinese: \"\n",
        "promptGeneration(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM_eA6Ni-EXu",
        "outputId": "1c53b448-7df7-4937-9609-38c4a55a3d05"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你好！},\n",
            "    {English: \"How are you?\", Chinese: \"你好吗？\"},\n",
            "    {English: \"Fine, thank you.\", Chinese: \"很好，谢谢\"},\n",
            "    {English:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Expecting the answer for a given question\n",
        "\n",
        "Asking different questions to Language model for getting the answers"
      ],
      "metadata": {
        "id": "enpetOGO94Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Create a write up on MLOps\"\n",
        "promptGeneration(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pgqRhmW5z7Q",
        "outputId": "ac5ccfbe-2e83-4d64-f66b-faf124f72f54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "MLOps is a term for the practice of combining machine learning and software engineering practices. The goal of MLOps is to improve the quality and speed of machine learning development and deployment. \n",
            "\n",
            " MLOps is a set of practices that aim to improve the collaboration between data scientists and software engineers, and to make\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Explain me about singleton software design pattern\"\n",
        "promptGeneration(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfzBugue6A6h",
        "outputId": "8f09fb1b-c78d-47a9-908d-a0b1b443c1a6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The singleton software design pattern is a software design pattern that restricts the instantiation of a class to one object. This is useful when only one instance of a class is needed to coordinate actions across the system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding example\n",
        "\n",
        "Asking the language model to create the code for given scenarios"
      ],
      "metadata": {
        "id": "cL_s4O5k_uQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "codeScenario = \"write a SQL Query for generating top salary employee in each department\"\n",
        "promptGeneration(codeScenario)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmKnHEaHBTdo",
        "outputId": "363a83d5-9630-4cec-edca-6a586f35d0e5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "SELECT dept.name AS \"Department\", emp.name AS \"Employee\", MAX(salary) AS \"Highest Salary\"\n",
            "FROM employee emp\n",
            "JOIN department dept ON emp.dept_id = dept.id\n",
            "GROUP BY dept.name,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "codeScenario = \"write a basic hello world program in Java\"\n",
        "promptGeneration(codeScenario)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJs6axOQBWQa",
        "outputId": "d792aacd-d6af-41a7-e07a-c751aa1d8ee1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "public class HelloWorld {\n",
            "    public static void main(String[] args) {\n",
            "        System.out.println(\"Hello, World!\");\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working as ML Model\n",
        "\n",
        "Using the Language model as ML Model for sentiment classifier and celcius to farenheit converter model."
      ],
      "metadata": {
        "id": "M0BQyYLfB5KO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scenario = \"\"\"Decide whether the sentence's sentiment is positive, neutral, or negative.\n",
        "\n",
        "Tweet: \"I loved the new Batman movie!\"\n",
        "Sentiment:\"\"\"\n",
        "promptGeneration(scenario)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSsMTuXYBoJo",
        "outputId": "1899649f-ef50-4bad-8a03-cbc1640223d2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"0 celcius:32 farenheit, 5 celcius:41 farenheit,10 celcius:50 farenheit, 15 celcius: \"\"\"\n",
        "promptGeneration(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix0-SQAfDGX8",
        "outputId": "bf08d0bf-c948-4136-e64b-2a69d88842f2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 59 farenheit, 20 celcius:  68 farenheit, 25 celcius:  77 farenheit, 30 celcius:  86 farenheit, 35 celcius:  95 farenheit, 40 celcius:  104 farenheit, 45 celcius:  113 farenheit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KiqixiLJDkGF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}