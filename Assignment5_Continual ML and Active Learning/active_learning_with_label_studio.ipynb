{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpYzn0p24h89i0z6zWSLMD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukkatharun/advance-deep-learning-assignments/blob/main/Assignment5_Continual%20ML%20and%20Active%20Learning/active_learning_with_label_studio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dx_CHfpU2Dpa",
        "outputId": "7d8eb215-03b2-402c-e715-18b76888ddb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lightly\n",
            "  Downloading lightly-1.2.35-py3-none-any.whl (507 kB)\n",
            "\u001b[K     |████████████████████████████████| 507 kB 14.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from lightly) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.7/dist-packages (from lightly) (4.64.1)\n",
            "Collecting pytorch-lightning>=1.0.4\n",
            "  Downloading pytorch_lightning-1.8.1-py3-none-any.whl (798 kB)\n",
            "\u001b[K     |████████████████████████████████| 798 kB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from lightly) (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from lightly) (1.21.6)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.7/dist-packages (from lightly) (57.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from lightly) (0.13.1+cu113)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from lightly) (2.23.0)\n",
            "Collecting lightly-utils~=0.0.0\n",
            "  Downloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.7/dist-packages (from lightly) (2022.9.24)\n",
            "Collecting hydra-core>=1.0.0\n",
            "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from lightly) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.0.0->lightly) (21.3)\n",
            "Collecting omegaconf~=2.2\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.0.0->lightly) (5.10.0)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 71.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from lightly-utils~=0.0.0->lightly) (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf~=2.2->hydra-core>=1.0.0->lightly) (6.0)\n",
            "Collecting lightning-utilities==0.3.*\n",
            "  Downloading lightning_utilities-0.3.0-py3-none-any.whl (15 kB)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.10.2-py3-none-any.whl (529 kB)\n",
            "\u001b[K     |████████████████████████████████| 529 kB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.4->lightly) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.4->lightly) (4.1.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.4->lightly) (2.9.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.4->lightly) (2022.10.0)\n",
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (3.8.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (6.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->hydra-core>=1.0.0->lightly) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->lightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->lightly) (2.10)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (1.50.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (2.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (1.3.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (0.38.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (0.6.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (3.19.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.0.4->lightly) (3.2.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning>=1.0.4->lightly) (2.1.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, fire\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144576 sha256=dcf4b4c29eae9fe0c256c9cc50f705ed0eac9d64bb310a0c3b7e6b9e7da9c7a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115940 sha256=ec42f0e6bc86909a2dc7dd9d8d08edb56e781b1d5e6540f1418a1c93e0c12211\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built antlr4-python3-runtime fire\n",
            "Installing collected packages: fire, antlr4-python3-runtime, torchmetrics, omegaconf, lightning-utilities, pytorch-lightning, lightly-utils, hydra-core, lightly\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 fire-0.4.0 hydra-core-1.2.0 lightly-1.2.35 lightly-utils-0.0.2 lightning-utilities-0.3.0 omegaconf-2.2.3 pytorch-lightning-1.8.1 torchmetrics-0.10.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install lightly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings"
      ],
      "metadata": {
        "id": "SFMXBUkpDGUG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lightly-magic token='3188e34d5bc6c77b6b8c58e62c2b3676a68ee5e989e53999' dataset_id='63747a5096ba509f0cd0aa55' input_dir='/content/training_data' trainer.max_epochs=2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LWlGD1P_Yo5",
        "outputId": "711f3ba8-bec0-47e7-dbd2-b0b5658e5eed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/hydra/_internal/hydra.py:127: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  configure_logging=with_log_configuration,\n",
            "########## Starting to train an embedding model.\n",
            "/usr/local/lib/python3.7/dist-packages/lightly/cli/train_cli.py:69: UserWarning: Training a self-supervised model with a small batch size: 16! Small batch size may harm embedding quality. You can specify the batch size via the loader key-word: loader.batch_size=BSZ\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://storage.googleapis.com/models_boris/whattolabel-resnet18-simclr-d32-w1.0-i-085d0693.pth\" to /root/.cache/torch/hub/checkpoints/whattolabel-resnet18-simclr-d32-w1.0-i-085d0693.pth\n",
            "100% 42.8M/42.8M [00:00<00:00, 222MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
            "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: /content/lightly_outputs/2022-11-16/05-55-42/lightning_logs\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:601: UserWarning: Checkpoint directory /content/lightly_outputs/2022-11-16/05-55-42 exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type       | Params\n",
            "-----------------------------------------\n",
            "0 | model     | _SimCLR    | 11.2 M\n",
            "1 | criterion | NTXentLoss | 0     \n",
            "-----------------------------------------\n",
            "11.2 M    Trainable params\n",
            "0         Non-trainable params\n",
            "11.2 M    Total params\n",
            "44.762    Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1562: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  category=PossibleUserWarning,\n",
            "Epoch 1: 100% 4/4 [00:00<00:00,  5.25it/s, loss=3.06, v_num=0]`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "Epoch 1: 100% 4/4 [00:01<00:00,  3.28it/s, loss=3.06, v_num=0]\n",
            "Best model is stored at: \u001b[94m/content/lightly_outputs/2022-11-16/05-55-42/lightly_epoch_1.ckpt\u001b[0m\n",
            "########## Starting to embed your dataset.\n",
            "Compute efficiency: 0.53: 100% 68/68 [00:00<00:00, 384.92imgs/s]\n",
            "Embeddings are stored at \u001b[94m/content/lightly_outputs/2022-11-16/05-55-42/embeddings.csv\u001b[0m\n",
            "########## Starting to upload your dataset to the Lightly platform.\n",
            "Uploading \u001b[92m68\u001b[0m images (with \u001b[92m2\u001b[0m workers).\n",
            "100% 68/68 [00:12<00:00,  5.47imgs/s]\n",
            "Finished the upload of the dataset.\n",
            "Finished upload of embeddings.\n",
            "########## Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, io, models, ops, transforms, utils"
      ],
      "metadata": {
        "id": "_GhOYZESAdZe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "\n",
        "def read_label_element(label_element: Dict) -> Tuple[str, str]:\n",
        "    filepath = \"/\" + label_element[\"data\"][\"image\"].split(\"?d=\")[-1]\n",
        "    label = label_element[\"annotations\"][0][\"result\"][0][\"value\"][\"rectanglelabels\"][0]\n",
        "    return filepath, label\n",
        "\n",
        "def read_LabelStudio_label_file(filepath: str) -> Tuple[List[str],List[str]]:\n",
        "    with open(filepath, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    filepaths, labels = zip(* [read_label_element(label_element) for label_element in data])\n",
        "    return filepaths, labels"
      ],
      "metadata": {
        "id": "DZeAn1XRAkCu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from lightly.active_learning.scorers import ScorerClassification\n",
        "from PIL import Image\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms as T\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class TorchImageDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_paths: List[str],\n",
        "        label_names: List[str],\n",
        "        transform: object = None,\n",
        "        labels: List[str] = None,\n",
        "    ):\n",
        "        if transform is None:\n",
        "            transform = T.Compose(\n",
        "                [\n",
        "                    T.Resize((360, 117)),\n",
        "                    T.ToTensor(),\n",
        "                    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                ]\n",
        "            )\n",
        "        self.transform = transform\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "\n",
        "        self.label_names = label_names\n",
        "        self.label_name_to_index = {name: i for i, name in enumerate(label_names)}\n",
        "\n",
        "    def __getitem__(self, index: int) -> Union[Tuple[torch.Tensor, int], torch.Tensor]:\n",
        "        image_path = self.image_paths[index]\n",
        "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "        image_torch = self.transform(image_pil)\n",
        "        if self.labels:\n",
        "            return image_torch, self.label_name_to_index[self.labels[index]]\n",
        "        else:\n",
        "            return image_torch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "\n",
        "class ClassificationModel:\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_classes: int = 3, \n",
        "        no_epochs: int = 5, \n",
        "        num_workers: int = None,\n",
        "        batch_size: int = 1,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.model = torchvision.models.resnet18(\n",
        "            pretrained=False, progress=True, num_classes=num_classes\n",
        "        )\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = torch.device(device)\n",
        "        self.model.to(self.device)\n",
        "        self.no_epochs = no_epochs\n",
        "        self.model_is_trained = False\n",
        "        self.num_workers = num_workers or min(8, os.cpu_count())\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def save_on_disk(self, model_path: str = \"./classifier.pth\"):\n",
        "        to_save = {\"model\": self.model.to(\"cpu\"), \"label_names\": self.label_names}\n",
        "        torch.save(to_save, model_path)\n",
        "\n",
        "    def load_from_disk(self, model_path: str = \"./classifier.pth\"):\n",
        "        saved_data = torch.load(model_path)\n",
        "        self.label_names = saved_data[\"label_names\"]\n",
        "        self.model = saved_data[\"model\"].to(self.device)\n",
        "        self.model_is_trained = True\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        image_paths: List[str],\n",
        "        image_labels: List[str],\n",
        "        label_names: List[str] = None,\n",
        "    ):\n",
        "        print(\"STARTING FITTING\")\n",
        "        if label_names is None:\n",
        "            self.label_names = sorted(list(set(image_labels)))\n",
        "\n",
        "        dataset = TorchImageDataset(\n",
        "            image_paths=image_paths, label_names=self.label_names, labels=image_labels\n",
        "        )\n",
        "        \n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset, \n",
        "            batch_size=self.batch_size, \n",
        "            shuffle=True, \n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
        "        self.model.train()\n",
        "        pbar = tqdm(range(self.no_epochs), file=sys.stdout)\n",
        "        for epoch in pbar:  \n",
        "            running_loss = 0.0\n",
        "            total_samples = 0\n",
        "            correct = 0\n",
        "            for data in dataloader:\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * labels.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_samples += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            text = f\"epoch: {epoch} loss: {running_loss / total_samples:.6f} accuracy: {correct/total_samples:.3f}\"\n",
        "            tqdm.write(text)\n",
        "\n",
        "        self.model_is_trained = True\n",
        "        print(\"FINISHED FITTING\")\n",
        "\n",
        "    def predict(self, image_paths: List[str]) -> Tuple[List[str], ScorerClassification]:\n",
        "        print(\"STARTING PREDICTION\")\n",
        "\n",
        "        if not self.model_is_trained:\n",
        "            raise ValueError\n",
        "\n",
        "        dataset = TorchImageDataset(image_paths, self.label_names)\n",
        "\n",
        "        dataloader = DataLoader(\n",
        "            dataset, \n",
        "            batch_size=self.batch_size, \n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "        predictions = []\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x in tqdm(dataloader):\n",
        "                pred = self.model(x.to(self.device)).cpu()\n",
        "                predictions.append(pred)\n",
        "\n",
        "        print(\"PUTTING TOGETHER RETURN VALUES\")\n",
        "\n",
        "        predictions = [i for sublist in predictions for i in sublist]\n",
        "        predictions = torch.stack(predictions, dim=0)\n",
        "        predictions = torch.nn.functional.softmax(predictions, dim=1)\n",
        "\n",
        "        predicted_classes_int = torch.argmax(predictions, dim=1)\n",
        "        predicted_classes_str = [self.label_names[i] for i in predicted_classes_int]\n",
        "        scorer = ScorerClassification(predictions)\n",
        "\n",
        "        print(\"FINISHED PREDICTION\")\n",
        "\n",
        "        return predicted_classes_str, scorer"
      ],
      "metadata": {
        "id": "baI5XqNFAls0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from lightly.active_learning.agents import ActiveLearningAgent\n",
        "from lightly.active_learning.config import SelectionConfig\n",
        "from lightly.api import ApiWorkflowClient\n",
        "from lightly.data import LightlyDataset\n",
        "from lightly.openapi_generated.swagger_client import SamplingMethod\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    path_full_dataset = \"/content/training_data\" \n",
        "    lighty_webapp_token = \"3188e34d5bc6c77b6b8c58e62c2b3676a68ee5e989e53999\"\n",
        "    lighty_webapp_dataset_id = \"63747a5096ba509f0cd0aa55\"\n",
        "    api_workflow_client = ApiWorkflowClient(token=lighty_webapp_token, dataset_id=lighty_webapp_dataset_id)\n",
        "    al_agent = ActiveLearningAgent(api_workflow_client=api_workflow_client)\n",
        "\n",
        "    filepaths, labels = read_LabelStudio_label_file(\"/content/updated_labelled.json\") ##UPDATE THIS PATH\n",
        "\n",
        "    classifier = ClassificationModel(no_epochs=20)\n",
        "\n",
        "    classifier.fit(image_paths=filepaths, image_labels=labels)\n",
        "\n",
        "    #Predict with the classifier on the complete dataset\n",
        "    image_filenames_full_dataset = LightlyDataset(path_full_dataset).get_filenames()\n",
        "    image_paths_full_dataset = [os.path.join(path_full_dataset, filename) for filename in image_filenames_full_dataset]\n",
        "    predicted_classes_str, scorer = classifier.predict(image_paths=image_paths_full_dataset)\n",
        "\n",
        "    # Use the active learning sampler \"CORAL\" via the Lightly API to sample until we have 45 samples\n",
        "    sampler_config = SelectionConfig(method=SamplingMethod.CORAL, n_samples=45, name=\"special_assignment\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v_bfXSnAnW7",
        "outputId": "0ac47e02-7921-4e1c-92cc-742136c4cf93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING FITTING\n",
            "epoch: 0 loss: 0.811690 accuracy: 0.458\n",
            "epoch: 1 loss: 1.765664 accuracy: 0.375\n",
            "epoch: 2 loss: 0.961020 accuracy: 0.625\n",
            "epoch: 3 loss: 0.967972 accuracy: 0.542\n",
            "epoch: 4 loss: 1.127668 accuracy: 0.542\n",
            "epoch: 5 loss: 0.561273 accuracy: 0.708\n",
            "epoch: 6 loss: 0.480842 accuracy: 0.750\n",
            "epoch: 7 loss: 0.492746 accuracy: 0.875\n",
            "epoch: 8 loss: 0.440549 accuracy: 0.750\n",
            "epoch: 9 loss: 0.464239 accuracy: 0.833\n",
            "epoch: 10 loss: 0.835674 accuracy: 0.667\n",
            "epoch: 11 loss: 0.485023 accuracy: 0.833\n",
            "epoch: 12 loss: 0.311716 accuracy: 0.833\n",
            "epoch: 13 loss: 0.351937 accuracy: 0.792\n",
            "epoch: 14 loss: 0.429462 accuracy: 0.875\n",
            "epoch: 15 loss: 0.554824 accuracy: 0.792\n",
            "epoch: 16 loss: 0.656399 accuracy: 0.875\n",
            "epoch: 17 loss: 0.449147 accuracy: 0.708\n",
            "epoch: 18 loss: 0.607674 accuracy: 0.750\n",
            "epoch: 19 loss: 0.334218 accuracy: 0.833\n",
            "100%|██████████| 20/20 [00:10<00:00,  1.85it/s]\n",
            "FINISHED FITTING\n",
            "STARTING PREDICTION\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68/68 [00:00<00:00, 121.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PUTTING TOGETHER RETURN VALUES\n",
            "FINISHED PREDICTION\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6U8vJBl4BJUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}