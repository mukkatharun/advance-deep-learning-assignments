{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras and pytorch examples using github copilot.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPvfgDKw20YmV5sCsxuLon9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukkatharun/advance-machine-learning-assignments/blob/main/assignment1GithubCopilot/keras_and_pytorch_examples_using_github_copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras Example"
      ],
      "metadata": {
        "id": "vk3bHYyMM8bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first neural network with keras tutorial\n",
        "from numpy import loadtxt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "# load the dataset\n",
        "dataset = loadtxt('/content/pima-indians-diabetes.data.csv', delimiter=',')\n",
        "# split into input (X) and output (y) variables\n",
        "X = dataset[:,0:8]\n",
        "y = dataset[:,8]\n",
        "# create the model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=50, batch_size=10)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, y)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "# calculate predictions\n",
        "predictions = model.predict(X)\n",
        "# round predictions\n",
        "rounded = [round(x[0]) for x in predictions]\n",
        "print(rounded)\n",
        "print(y)\n",
        "\n",
        "# I am having issues with tensorflow in my local machine, so running the generated code in colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnzDDXsHJj4G",
        "outputId": "d6d080d2-5f80-4d5b-8a3b-7ce551723731"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "77/77 [==============================] - 1s 2ms/step - loss: 6.7741 - accuracy: 0.6263\n",
            "Epoch 2/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.4922 - accuracy: 0.6471\n",
            "Epoch 3/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.3742 - accuracy: 0.6133\n",
            "Epoch 4/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.1902 - accuracy: 0.6172\n",
            "Epoch 5/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.1770 - accuracy: 0.6107\n",
            "Epoch 6/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.0540 - accuracy: 0.6367\n",
            "Epoch 7/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.0770 - accuracy: 0.6185\n",
            "Epoch 8/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.0609 - accuracy: 0.6367\n",
            "Epoch 9/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.9475 - accuracy: 0.6211\n",
            "Epoch 10/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.9462 - accuracy: 0.6224\n",
            "Epoch 11/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.9260 - accuracy: 0.6211\n",
            "Epoch 12/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.9296 - accuracy: 0.6341\n",
            "Epoch 13/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.8777 - accuracy: 0.6263\n",
            "Epoch 14/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.8501 - accuracy: 0.6510\n",
            "Epoch 15/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.8121 - accuracy: 0.6628\n",
            "Epoch 16/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7823 - accuracy: 0.6419\n",
            "Epoch 17/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7706 - accuracy: 0.6536\n",
            "Epoch 18/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7543 - accuracy: 0.6719\n",
            "Epoch 19/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7088 - accuracy: 0.6510\n",
            "Epoch 20/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7324 - accuracy: 0.6602\n",
            "Epoch 21/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7072 - accuracy: 0.6836\n",
            "Epoch 22/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.6875\n",
            "Epoch 23/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7115 - accuracy: 0.6784\n",
            "Epoch 24/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6668 - accuracy: 0.6875\n",
            "Epoch 25/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7218 - accuracy: 0.6732\n",
            "Epoch 26/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6535 - accuracy: 0.6914\n",
            "Epoch 27/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6481 - accuracy: 0.6849\n",
            "Epoch 28/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6445 - accuracy: 0.7135\n",
            "Epoch 29/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6200 - accuracy: 0.7174\n",
            "Epoch 30/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5969 - accuracy: 0.7174\n",
            "Epoch 31/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6086 - accuracy: 0.7305\n",
            "Epoch 32/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6216 - accuracy: 0.7096\n",
            "Epoch 33/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6360 - accuracy: 0.7057\n",
            "Epoch 34/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6182 - accuracy: 0.7005\n",
            "Epoch 35/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5998 - accuracy: 0.7240\n",
            "Epoch 36/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6249 - accuracy: 0.7044\n",
            "Epoch 37/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6076 - accuracy: 0.7070\n",
            "Epoch 38/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5988 - accuracy: 0.6992\n",
            "Epoch 39/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6149 - accuracy: 0.7083\n",
            "Epoch 40/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6192 - accuracy: 0.6979\n",
            "Epoch 41/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6004 - accuracy: 0.7096\n",
            "Epoch 42/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6080 - accuracy: 0.7409\n",
            "Epoch 43/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6539 - accuracy: 0.6966\n",
            "Epoch 44/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5703 - accuracy: 0.7214\n",
            "Epoch 45/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6109 - accuracy: 0.7031\n",
            "Epoch 46/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5758 - accuracy: 0.7279\n",
            "Epoch 47/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5909 - accuracy: 0.7096\n",
            "Epoch 48/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.7461\n",
            "Epoch 49/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5852 - accuracy: 0.7279\n",
            "Epoch 50/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5651 - accuracy: 0.7279\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.6125 - accuracy: 0.6901\n",
            "\n",
            "accuracy: 69.01%\n",
            "[1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0]\n",
            "[1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
            " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
            " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
            " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
            " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
            " 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
            " 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
            " 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7dVdVlupJvPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch exmaple"
      ],
      "metadata": {
        "id": "_TveNRE_M63U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch mnist example\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "#load the mnist dataset and split into train and test sets\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "# we can see the copilot can give the code completion for our task\n",
        "\n",
        "\n",
        "# print the dataset shapes\n",
        "print(train_dataset.train_data.size())\n",
        "print(train_dataset.train_labels.size())\n",
        "print(test_dataset.test_data.size())\n",
        "print(test_dataset.test_labels.size())\n",
        "\n",
        "#create the pytorch dataloaders\n",
        "batch_size = 100\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "#create the model\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "    \n",
        "model = Net()\n",
        "\n",
        "#initialize the weights\n",
        "init.xavier_uniform(model.conv1.weight)\n",
        "init.xavier_uniform(model.conv2.weight)\n",
        "init.xavier_uniform(model.fc1.weight)\n",
        "init.xavier_uniform(model.fc2.weight)\n",
        "\n",
        "\n",
        "#create the loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "\n",
        "#train the model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                  %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data))\n",
        "\n",
        "\n",
        "#test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in test_loader:\n",
        "    images = Variable(images)\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "\n",
        "#similarly i have generated the overall code using the copilot and running it in colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eeeGiyVMIX1",
        "outputId": "181da842-69c7-41f6-ef0a-26c36a9592fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([60000])\n",
            "torch.Size([10000, 28, 28])\n",
            "torch.Size([10000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 2.1120\n",
            "Epoch [1/5], Step [200/600], Loss: 1.6954\n",
            "Epoch [1/5], Step [300/600], Loss: 1.3969\n",
            "Epoch [1/5], Step [400/600], Loss: 1.1058\n",
            "Epoch [1/5], Step [500/600], Loss: 0.8592\n",
            "Epoch [1/5], Step [600/600], Loss: 0.8529\n",
            "Epoch [2/5], Step [100/600], Loss: 0.9393\n",
            "Epoch [2/5], Step [200/600], Loss: 0.8359\n",
            "Epoch [2/5], Step [300/600], Loss: 0.7045\n",
            "Epoch [2/5], Step [400/600], Loss: 0.6143\n",
            "Epoch [2/5], Step [500/600], Loss: 0.4340\n",
            "Epoch [2/5], Step [600/600], Loss: 0.4112\n",
            "Epoch [3/5], Step [100/600], Loss: 0.6539\n",
            "Epoch [3/5], Step [200/600], Loss: 0.4612\n",
            "Epoch [3/5], Step [300/600], Loss: 0.4501\n",
            "Epoch [3/5], Step [400/600], Loss: 0.7137\n",
            "Epoch [3/5], Step [500/600], Loss: 0.5592\n",
            "Epoch [3/5], Step [600/600], Loss: 0.5915\n",
            "Epoch [4/5], Step [100/600], Loss: 0.4477\n",
            "Epoch [4/5], Step [200/600], Loss: 0.4735\n",
            "Epoch [4/5], Step [300/600], Loss: 0.4278\n",
            "Epoch [4/5], Step [400/600], Loss: 0.5001\n",
            "Epoch [4/5], Step [500/600], Loss: 0.4345\n",
            "Epoch [4/5], Step [600/600], Loss: 0.3483\n",
            "Epoch [5/5], Step [100/600], Loss: 0.3999\n",
            "Epoch [5/5], Step [200/600], Loss: 0.3504\n",
            "Epoch [5/5], Step [300/600], Loss: 0.2824\n",
            "Epoch [5/5], Step [400/600], Loss: 0.3838\n",
            "Epoch [5/5], Step [500/600], Loss: 0.2838\n",
            "Epoch [5/5], Step [600/600], Loss: 0.2544\n",
            "Accuracy of the network on the 10000 test images: 89 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMovusw7ObF5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}